{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import random\n",
    "# import seaborn as sns\n",
    "import os.path as path\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt # graphs plotting\n",
    "from Bio import SeqIO # some BioPython that will come in handy\n",
    "#matplotlib inline\n",
    "import numpy\n",
    "import csv \n",
    "\n",
    "from matplotlib import rc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from numpy import mean\n",
    "\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import statistics\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "import math\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# for Arial typefont\n",
    "matplotlib.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "\n",
    "## for Palatino and other serif fonts use:\n",
    "# rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "# matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "\n",
    "## for LaTeX typefont\n",
    "# matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "# matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "## for another LaTeX typefont\n",
    "# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "\n",
    "# rc('text', usetex = True)\n",
    "\n",
    "print(\"Packages imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 6\n",
      "1 / 6\n",
      "2 / 6\n",
      "3 / 6\n",
      "4 / 6\n",
      "5 / 6\n",
      "6 / 6\n"
     ]
    }
   ],
   "source": [
    "dataset_save_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/dataset/\"\n",
    "# country_name = ['California', 'England', 'Denmark', 'Germany', 'Japan', 'Sweden', 'Canada', 'India', 'Australia', 'Norway', \n",
    "#                      'Israel', 'Greece', 'Turkey', 'Iceland', 'Mexico', 'Brazil', \n",
    "#                      'Russia', 'Singapore', 'Qatar', 'Bangladesh', 'Colombia', \n",
    "#                      'Peru', 'Egypt', 'Malaysia', 'Argentina']\n",
    "\n",
    "\n",
    "\n",
    "country_name = ['India', 'California', 'England', 'Germany', 'Brazil', 'Argentina', 'Qatar']\n",
    "\n",
    "total_reduces_seqs = 600\n",
    "Trial = 3\n",
    "data_reduced = []\n",
    "data_labels_reduced = []\n",
    "    \n",
    "for ind in range(0,len(country_name)):\n",
    "    print(ind,\"/\",len(country_name)-1)\n",
    "    temp_name = country_name[ind]\n",
    "    seq_file_small = []\n",
    "    with open(dataset_save_path + temp_name + \"_covid_protein_sequences_temp.csv\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            seq_file_small.append(', '.join(row))\n",
    "            \n",
    "\n",
    "#     print(len(seq_file_small[0]))\n",
    "    \n",
    "    xx = random.sample(range(0,len(seq_file_small)), total_reduces_seqs)\n",
    "    red_data = [seq_file_small[i] for i in xx]\n",
    "    \n",
    "    red_data_label = []\n",
    "    for inn in range(0,total_reduces_seqs):\n",
    "        red_data_label.append(country_name[ind])\n",
    "    \n",
    "    if(ind==0):\n",
    "        data_reduced = red_data\n",
    "        data_labels_reduced = red_data_label\n",
    "    else:\n",
    "        data_reduced = data_reduced + red_data\n",
    "        data_labels_reduced = data_labels_reduced + red_data_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Complete Reduced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_name_tmp = \"New_reduced_data\"\n",
    "\n",
    "with open(dataset_save_path + temp_name_tmp + \"_covid_protein_sequences_trial_\" + str(Trial) + \".csv\", 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for i in range(0,len(data_reduced)):\n",
    "        aa = (data_reduced[i].replace(',', ''))\n",
    "        aaa = aa.replace(' ','')\n",
    "        writer.writerow(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4200"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/Kernel_k8_m2_Alphabet_Size21.txt\"\n",
    "\n",
    "seq_file_small = []\n",
    "with open(read_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        seq_file_small.append(', '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_jernel_mat = []\n",
    "mat_2 = []\n",
    "for y in range(0,len(seq_file_small)):\n",
    "    aa = seq_file_small[y].split(\" \")\n",
    "    tmp_list = []\n",
    "    for yy in range(0,len(aa)-1):\n",
    "        tmp_list.append(int(aa[yy]))\n",
    "    \n",
    "    final_jernel_mat.append(tmp_list)\n",
    "#     mat_2.append(tmp_list)\n",
    "# len(aa)\n",
    "# [Python] Agglomerative Clustering Similarity Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering with Kernel Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =>4, m =>0, Trail =>1, # of Clusters =>3\n",
      "k =>4, m =>0, Trail =>1, # of Clusters =>4\n",
      "k =>4, m =>0, Trail =>1, # of Clusters =>5\n",
      "k =>4, m =>0, Trail =>1, # of Clusters =>6\n",
      "k =>4, m =>1, Trail =>1, # of Clusters =>3\n",
      "k =>4, m =>1, Trail =>1, # of Clusters =>4\n",
      "k =>4, m =>1, Trail =>1, # of Clusters =>5\n",
      "k =>4, m =>1, Trail =>1, # of Clusters =>6\n",
      "k =>4, m =>2, Trail =>1, # of Clusters =>3\n",
      "k =>4, m =>2, Trail =>1, # of Clusters =>4\n",
      "k =>4, m =>2, Trail =>1, # of Clusters =>5\n",
      "k =>4, m =>2, Trail =>1, # of Clusters =>6\n",
      "k =>4, m =>3, Trail =>1, # of Clusters =>3\n",
      "k =>4, m =>3, Trail =>1, # of Clusters =>4\n",
      "k =>4, m =>3, Trail =>1, # of Clusters =>5\n",
      "k =>4, m =>3, Trail =>1, # of Clusters =>6\n",
      "k =>6, m =>0, Trail =>1, # of Clusters =>3\n",
      "k =>6, m =>0, Trail =>1, # of Clusters =>4\n",
      "k =>6, m =>0, Trail =>1, # of Clusters =>5\n",
      "k =>6, m =>0, Trail =>1, # of Clusters =>6\n",
      "k =>6, m =>1, Trail =>1, # of Clusters =>3\n",
      "k =>6, m =>1, Trail =>1, # of Clusters =>4\n",
      "k =>6, m =>1, Trail =>1, # of Clusters =>5\n",
      "k =>6, m =>1, Trail =>1, # of Clusters =>6\n",
      "k =>6, m =>2, Trail =>1, # of Clusters =>3\n",
      "k =>6, m =>2, Trail =>1, # of Clusters =>4\n",
      "k =>6, m =>2, Trail =>1, # of Clusters =>5\n",
      "k =>6, m =>2, Trail =>1, # of Clusters =>6\n",
      "k =>6, m =>3, Trail =>1, # of Clusters =>3\n",
      "k =>6, m =>3, Trail =>1, # of Clusters =>4\n",
      "k =>6, m =>3, Trail =>1, # of Clusters =>5\n",
      "k =>6, m =>3, Trail =>1, # of Clusters =>6\n",
      "k =>8, m =>0, Trail =>1, # of Clusters =>3\n",
      "k =>8, m =>0, Trail =>1, # of Clusters =>4\n",
      "k =>8, m =>0, Trail =>1, # of Clusters =>5\n",
      "k =>8, m =>0, Trail =>1, # of Clusters =>6\n",
      "k =>8, m =>1, Trail =>1, # of Clusters =>3\n",
      "k =>8, m =>1, Trail =>1, # of Clusters =>4\n",
      "k =>8, m =>1, Trail =>1, # of Clusters =>5\n",
      "k =>8, m =>1, Trail =>1, # of Clusters =>6\n",
      "k =>8, m =>2, Trail =>1, # of Clusters =>3\n",
      "k =>8, m =>2, Trail =>1, # of Clusters =>4\n",
      "k =>8, m =>2, Trail =>1, # of Clusters =>5\n",
      "k =>8, m =>2, Trail =>1, # of Clusters =>6\n",
      "k =>8, m =>3, Trail =>1, # of Clusters =>3\n",
      "k =>8, m =>3, Trail =>1, # of Clusters =>4\n",
      "k =>8, m =>3, Trail =>1, # of Clusters =>5\n",
      "k =>8, m =>3, Trail =>1, # of Clusters =>6\n",
      "k =>4, m =>0, Trail =>2, # of Clusters =>3\n",
      "k =>4, m =>0, Trail =>2, # of Clusters =>4\n",
      "k =>4, m =>0, Trail =>2, # of Clusters =>5\n",
      "k =>4, m =>0, Trail =>2, # of Clusters =>6\n",
      "k =>4, m =>1, Trail =>2, # of Clusters =>3\n",
      "k =>4, m =>1, Trail =>2, # of Clusters =>4\n",
      "k =>4, m =>1, Trail =>2, # of Clusters =>5\n",
      "k =>4, m =>1, Trail =>2, # of Clusters =>6\n",
      "k =>4, m =>2, Trail =>2, # of Clusters =>3\n",
      "k =>4, m =>2, Trail =>2, # of Clusters =>4\n",
      "k =>4, m =>2, Trail =>2, # of Clusters =>5\n",
      "k =>4, m =>2, Trail =>2, # of Clusters =>6\n",
      "k =>4, m =>3, Trail =>2, # of Clusters =>3\n",
      "k =>4, m =>3, Trail =>2, # of Clusters =>4\n",
      "k =>4, m =>3, Trail =>2, # of Clusters =>5\n",
      "k =>4, m =>3, Trail =>2, # of Clusters =>6\n",
      "k =>6, m =>0, Trail =>2, # of Clusters =>3\n",
      "k =>6, m =>0, Trail =>2, # of Clusters =>4\n",
      "k =>6, m =>0, Trail =>2, # of Clusters =>5\n",
      "k =>6, m =>0, Trail =>2, # of Clusters =>6\n",
      "k =>6, m =>1, Trail =>2, # of Clusters =>3\n",
      "k =>6, m =>1, Trail =>2, # of Clusters =>4\n",
      "k =>6, m =>1, Trail =>2, # of Clusters =>5\n",
      "k =>6, m =>1, Trail =>2, # of Clusters =>6\n",
      "k =>6, m =>2, Trail =>2, # of Clusters =>3\n",
      "k =>6, m =>2, Trail =>2, # of Clusters =>4\n",
      "k =>6, m =>2, Trail =>2, # of Clusters =>5\n",
      "k =>6, m =>2, Trail =>2, # of Clusters =>6\n",
      "k =>6, m =>3, Trail =>2, # of Clusters =>3\n",
      "k =>6, m =>3, Trail =>2, # of Clusters =>4\n",
      "k =>6, m =>3, Trail =>2, # of Clusters =>5\n",
      "k =>6, m =>3, Trail =>2, # of Clusters =>6\n",
      "k =>8, m =>0, Trail =>2, # of Clusters =>3\n",
      "k =>8, m =>0, Trail =>2, # of Clusters =>4\n",
      "k =>8, m =>0, Trail =>2, # of Clusters =>5\n",
      "k =>8, m =>0, Trail =>2, # of Clusters =>6\n",
      "k =>8, m =>1, Trail =>2, # of Clusters =>3\n",
      "k =>8, m =>1, Trail =>2, # of Clusters =>4\n",
      "k =>8, m =>1, Trail =>2, # of Clusters =>5\n",
      "k =>8, m =>1, Trail =>2, # of Clusters =>6\n",
      "k =>8, m =>2, Trail =>2, # of Clusters =>3\n",
      "k =>8, m =>2, Trail =>2, # of Clusters =>4\n",
      "k =>8, m =>2, Trail =>2, # of Clusters =>5\n",
      "k =>8, m =>2, Trail =>2, # of Clusters =>6\n",
      "k =>8, m =>3, Trail =>2, # of Clusters =>3\n",
      "k =>8, m =>3, Trail =>2, # of Clusters =>4\n",
      "k =>8, m =>3, Trail =>2, # of Clusters =>5\n",
      "k =>8, m =>3, Trail =>2, # of Clusters =>6\n"
     ]
    }
   ],
   "source": [
    "Final_agglomorative_confusion_matrix = []\n",
    "Number_of_clusters_arr = [3, 4, 5, 6]\n",
    "trial_arr = [1,2]\n",
    "k_value_arr = [4,6,8]\n",
    "m_value_arr = [0,1,2,3]\n",
    "\n",
    "for data_trials_ind in range(len(trial_arr)):\n",
    "    for k_val_ind in range(len(k_value_arr)):\n",
    "        for m_val_ind in range(len(m_value_arr)):\n",
    "            for number_clust_ind in range(len(Number_of_clusters_arr)):\n",
    "            \n",
    "                Number_of_clusters = Number_of_clusters_arr[number_clust_ind]\n",
    "                trial = trial_arr[data_trials_ind]\n",
    "                k_value = k_value_arr[k_val_ind]\n",
    "                m_value = m_value_arr[m_val_ind]\n",
    "\n",
    "                holdup_string = \"k =>\" + str(k_value) + \", m =>\" +  str(m_value) + \", Trail =>\" + str(trial) + \", # of Clusters =>\" + str(Number_of_clusters)\n",
    "                print(holdup_string)\n",
    "\n",
    "                #############  Read Kernel Matrix ################\n",
    "                read_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/Kernel_k\" + str(k_value) + \"_m\" + str(m_value) + \"_Alphabet_Size21_trial_\" + str(trial) + \".txt\"\n",
    "\n",
    "\n",
    "                seq_file_small = []\n",
    "                with open(read_path) as csv_file:\n",
    "                    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                    for row in csv_reader:\n",
    "                        seq_file_small.append(', '.join(row))\n",
    "                #############  Read Kernel Matrix ################\n",
    "\n",
    "                #############  Small Kernel Preprocessing ################\n",
    "                final_jernel_mat = []\n",
    "                mat_2 = []\n",
    "                for y in range(0,len(seq_file_small)):\n",
    "                    aa = seq_file_small[y].split(\" \")\n",
    "                    tmp_list = []\n",
    "                    for yy in range(0,len(aa)-1):\n",
    "                        tmp_list.append(int(aa[yy]))\n",
    "\n",
    "                    final_jernel_mat.append(tmp_list)\n",
    "                #############  Small Kernel Preprocessing ################\n",
    "\n",
    "                ################ Kernel Trick ######################\n",
    "                mat_2 = np.zeros((len(final_jernel_mat),len(final_jernel_mat)))\n",
    "\n",
    "                for i in range(0,len(final_jernel_mat)):\n",
    "                    tmp_2 = final_jernel_mat[i][i]\n",
    "                    for j in range(0,len(final_jernel_mat)):\n",
    "                        denom = math.sqrt(tmp_2 * final_jernel_mat[j][j])\n",
    "                        temp = final_jernel_mat[i][j] / (denom)\n",
    "                #         mat_2[i][j] = 1 - temp\n",
    "                        mat_2[i][j] = final_jernel_mat[i][i] + final_jernel_mat[j][j] - (2*(final_jernel_mat[i][j])) # distance kernel trick\n",
    "                ################ Kernel Trick ######################\n",
    "\n",
    "                ################ Agglomerative Clustering ######################\n",
    "                model = AgglomerativeClustering(affinity='precomputed', n_clusters=Number_of_clusters, linkage='average').fit(mat_2)\n",
    "                # print(model.labels_)\n",
    "                clusters_ids = model.labels_\n",
    "                ################ Agglomerative Clustering ######################\n",
    "\n",
    "                #############  Confusion Matrix ################\n",
    "\n",
    "                unique_labels = np.unique(data_labels_reduced)\n",
    "                labels_integers = []\n",
    "                for i in range(len(data_labels_reduced)):\n",
    "                    aa = (np.where(unique_labels==data_labels_reduced[i]))\n",
    "                    labels_integers.append(aa[0][0])\n",
    "\n",
    "                confusion_matrix(clusters_ids,labels_integers)\n",
    "\n",
    "                confuse = confusion_matrix(clusters_ids,labels_integers)\n",
    "\n",
    "                confuse_matrix = []\n",
    "                for i in range(0,len(confuse)):\n",
    "                    confuse_matrix.append(list(confuse[i]))\n",
    "                #############  Confusion Matrix ################\n",
    "\n",
    "                Final_agglomorative_confusion_matrix.append(confuse_matrix)\n",
    "                Final_agglomorative_confusion_matrix.append(holdup_string)\n",
    "#                 #############  Save Confusion Matrix ################\n",
    "#                 unique_labels = np.unique(data_labels_reduced)\n",
    "#                 dataset_save_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/results/Agglomerative Clustering/trial\" + str(trial) + \"/\"\n",
    "#                 with open(dataset_save_path + \"Agglomerative_Clustering_confusion_matrix\" + \"_numClust_\" + str(Number_of_clusters) + \"_dataTrial_\" + str(trial) + \"_k_value_\" + str(k_value) + \"_m_value_\" + str(m_value) +  \".csv\", 'w', newline='') as file:\n",
    "#                     writer = csv.writer(file)\n",
    "#                     for i in range(0,Number_of_clusters):#len(confuse)\n",
    "#                         if(i==0):\n",
    "#                             writer.writerow(np.array(unique_labels))\n",
    "#                         writer.writerow(np.array(confuse_matrix[i]))\n",
    "#                 #############  Save Confusion Matrix ################\n",
    "\n",
    "    #############  Save Confusion Matrix ################\n",
    "    unique_labels = np.unique(data_labels_reduced)\n",
    "    dataset_save_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/results/Agglomerative Clustering/trial\" + str(trial) + \"/\"\n",
    "    with open(dataset_save_path + \"Final_Agglomerative_Clustering_confusion_matrix.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(0,len(Final_agglomorative_confusion_matrix)):#len(confuse)\n",
    "            if(i==0):\n",
    "                writer.writerow(np.array(unique_labels))\n",
    "            if (i % 2) == 0:\n",
    "    #             print(\"{0} is Even number\". format(i))\n",
    "                tmp = Final_agglomorative_confusion_matrix[i]\n",
    "                for u in range(len(tmp)):\n",
    "                    writer.writerow(np.array(tmp[u]))\n",
    "            else:\n",
    "                writer.writerow((Final_agglomorative_confusion_matrix[i]))\n",
    "\n",
    "    #############  Save Confusion Matrix ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =>4, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>4, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>4, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>4, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>4, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>4, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>4, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>4, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>4, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>4, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>4, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>4, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>4, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>4, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>4, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>4, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>4, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>4, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>4, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>4, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>4, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>4, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>4, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>4, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>4, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>4, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>4, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>4, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>4, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>4, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>4, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>4, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>6, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>6, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>6, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>6, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>6, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>6, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>6, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>6, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>6, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>6, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>6, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>6, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>6, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>6, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>6, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>6, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>6, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>6, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>6, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>6, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>6, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>6, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>6, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>6, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>6, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>6, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>6, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>6, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>6, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>6, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>6, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>6, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>8, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>8, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>8, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>8, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>8, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>8, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>8, m =>0, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>8, m =>0, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>8, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>8, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>8, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>8, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>8, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>8, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>8, m =>1, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>8, m =>1, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>8, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>8, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>8, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>8, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>8, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>8, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>8, m =>2, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>8, m =>2, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>8, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>3\n",
      "k =>8, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>3\n",
      "k =>8, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>4\n",
      "k =>8, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>4\n",
      "k =>8, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>5\n",
      "k =>8, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>5\n",
      "k =>8, m =>3, Trail =>1, Principal Components =>3, # of Clusters =>6\n",
      "k =>8, m =>3, Trail =>1, Principal Components =>7, # of Clusters =>6\n",
      "k =>4, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>4, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>4, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>4, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>4, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>4, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>4, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>4, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>4, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>4, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>4, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>4, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>4, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>4, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>4, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>4, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>4, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>4, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>4, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>4, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>4, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>4, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>4, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>4, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>4, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>4, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>4, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>4, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>4, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>4, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>4, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>4, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>6, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>6, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>6, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>6, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>6, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>6, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>6, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>6, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>6, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>6, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>6, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>6, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>6, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>6, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>6, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>6, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>6, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>6, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>6, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>6, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>6, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>6, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>6, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>6, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>6, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>6, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>6, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>6, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>6, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>6, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>6, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>6, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>8, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>8, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>8, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>8, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>8, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>8, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>8, m =>0, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>8, m =>0, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>8, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>8, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>8, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>8, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>8, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>8, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>8, m =>1, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>8, m =>1, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>8, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>8, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>8, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>8, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>8, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>8, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>8, m =>2, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>8, m =>2, Trail =>2, Principal Components =>7, # of Clusters =>6\n",
      "k =>8, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>3\n",
      "k =>8, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>3\n",
      "k =>8, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>4\n",
      "k =>8, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>4\n",
      "k =>8, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>5\n",
      "k =>8, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>5\n",
      "k =>8, m =>3, Trail =>2, Principal Components =>3, # of Clusters =>6\n",
      "k =>8, m =>3, Trail =>2, Principal Components =>7, # of Clusters =>6\n"
     ]
    }
   ],
   "source": [
    "Final_agglomorative_confusion_matrix = []\n",
    "Number_of_clusters_arr = [3, 4, 5, 6]\n",
    "total_principal_components_arr = [3,7]\n",
    "trial_arr = [1,2]\n",
    "k_value_arr = [4,6,8]\n",
    "m_value_arr = [0,1,2,3]\n",
    "\n",
    "for data_trials_ind in range(len(trial_arr)):\n",
    "    for k_val_ind in range(len(k_value_arr)):\n",
    "        for m_val_ind in range(len(m_value_arr)):\n",
    "            for number_clust_ind in range(len(Number_of_clusters_arr)):\n",
    "                for principal_comp_ind in range(len(total_principal_components_arr)):\n",
    "                    \n",
    "                    Number_of_clusters = Number_of_clusters_arr[number_clust_ind]\n",
    "                    total_principal_components = total_principal_components_arr[principal_comp_ind]\n",
    "                    trial = trial_arr[data_trials_ind]\n",
    "                    k_value = k_value_arr[k_val_ind]\n",
    "                    m_value = m_value_arr[m_val_ind]\n",
    "                    \n",
    "                    holdup_string = \"k =>\" + str(k_value) + \", m =>\" +  str(m_value) + \", Trail =>\" + str(trial) + \", Principal Components =>\" + str(total_principal_components) + \", # of Clusters =>\" + str(Number_of_clusters)\n",
    "                    print(holdup_string)\n",
    "\n",
    "                    #############  Read Kernel Matrix ################\n",
    "                    read_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/Kernel_k\" + str(k_value) + \"_m\" + str(m_value) + \"_Alphabet_Size21_trial_\" + str(trial) + \".txt\"\n",
    "\n",
    "\n",
    "                    seq_file_small = []\n",
    "                    with open(read_path) as csv_file:\n",
    "                        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                        for row in csv_reader:\n",
    "                            seq_file_small.append(', '.join(row))\n",
    "                    #############  Read Kernel Matrix ################\n",
    "\n",
    "                    #############  Small Kernel Preprocessing ################\n",
    "                    final_jernel_mat = []\n",
    "                    mat_2 = []\n",
    "                    for y in range(0,len(seq_file_small)):\n",
    "                        aa = seq_file_small[y].split(\" \")\n",
    "                        tmp_list = []\n",
    "                        for yy in range(0,len(aa)-1):\n",
    "                            tmp_list.append(int(aa[yy]))\n",
    "\n",
    "                        final_jernel_mat.append(tmp_list)\n",
    "                    #############  Small Kernel Preprocessing ################\n",
    "\n",
    "                    #############  Kernel PCA ################\n",
    "                    transformer = KernelPCA(n_components=total_principal_components, kernel='precomputed')\n",
    "                    X_transformed = transformer.fit_transform(final_jernel_mat)\n",
    "                    X_transformed.shape\n",
    "                    #############  Kernel PCA ################\n",
    "\n",
    "                    #############  K-Means ################\n",
    "                    kmeans = KMeans(n_clusters=Number_of_clusters, random_state=0).fit(X_transformed)\n",
    "                    clusters_ids = kmeans.labels_\n",
    "                    #############  K-Means ################\n",
    "\n",
    "\n",
    "                    #############  Confusion Matrix ################\n",
    "                    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "                    unique_labels = np.unique(data_labels_reduced)\n",
    "                    labels_integers = []\n",
    "                    for i in range(len(data_labels_reduced)):\n",
    "                        aa = (np.where(unique_labels==data_labels_reduced[i]))\n",
    "                        labels_integers.append(aa[0][0])\n",
    "\n",
    "                    confusion_matrix(clusters_ids,labels_integers)\n",
    "\n",
    "                    confuse = confusion_matrix(clusters_ids,labels_integers)\n",
    "\n",
    "                    confuse_matrix = []\n",
    "                    for i in range(0,len(confuse)):\n",
    "                        confuse_matrix.append(list(confuse[i]))\n",
    "                    #############  Confusion Matrix ################\n",
    "\n",
    "                    Final_agglomorative_confusion_matrix.append(confuse_matrix)\n",
    "                    Final_agglomorative_confusion_matrix.append(holdup_string)\n",
    "#                     #############  Save Confusion Matrix ################\n",
    "#                     unique_labels = np.unique(data_labels_reduced)\n",
    "#                     dataset_save_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/results/Kerrnel PCA/trial\" + str(trial) + \"/\"\n",
    "#                     with open(dataset_save_path + \"Kernel_PCA_confusion_matrix\" + \"_numClust_\" + str(Number_of_clusters) + \"_num_principal_components_\" + str(total_principal_components) + \"_dataTrial_\" + str(trial) + \"_k_value_\" + str(k_value) + \"_m_value_\" + str(m_value) +  \".csv\", 'w', newline='') as file:\n",
    "#                         writer = csv.writer(file)\n",
    "#                         for i in range(0,Number_of_clusters):#len(confuse)\n",
    "#                             if(i==0):\n",
    "#                                 writer.writerow(np.array(unique_labels))\n",
    "#                             writer.writerow(np.array(confuse_matrix[i]))\n",
    "#                     #############  Save Confusion Matrix ################\n",
    "\n",
    "    #############  Save Confusion Matrix ################\n",
    "    unique_labels = np.unique(data_labels_reduced)\n",
    "    dataset_save_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/results/Kerrnel PCA/trial\" + str(trial) + \"/\"\n",
    "    with open(dataset_save_path + \"Final_Kerrnel_PCA_confusion_matrix.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(0,len(Final_agglomorative_confusion_matrix)):#len(confuse)\n",
    "            if(i==0):\n",
    "                writer.writerow(np.array(unique_labels))\n",
    "            if (i % 2) == 0:\n",
    "    #             print(\"{0} is Even number\". format(i))\n",
    "                tmp = Final_agglomorative_confusion_matrix[i]\n",
    "                for u in range(len(tmp)):\n",
    "                    writer.writerow(np.array(tmp[u]))\n",
    "            else:\n",
    "                writer.writerow((Final_agglomorative_confusion_matrix[i]))\n",
    "\n",
    "    #############  Save Confusion Matrix ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Kernel K-means\"\"\"\n",
    "\n",
    "# Author: Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n",
    "class KernelKMeans(BaseEstimator, ClusterMixin):\n",
    "    \"\"\"\n",
    "    Kernel K-means\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    Kernel k-means, Spectral Clustering and Normalized Cuts.\n",
    "    Inderjit S. Dhillon, Yuqiang Guan, Brian Kulis.\n",
    "    KDD 2004.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters=3, max_iter=50, tol=1e-3, random_state=None,\n",
    "                 kernel=\"linear\", gamma=None, degree=3, coef0=1,\n",
    "                 kernel_params=None, verbose=0):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.kernel_params = kernel_params\n",
    "        self.verbose = verbose\n",
    "        \n",
    "#     @property\n",
    "#     def _pairwise(self):\n",
    "#         print(\"This important Kernel Function is called\")\n",
    "#         return self.kernel == \"precomputed\"\n",
    "\n",
    "#     def _get_kernel(self, X, Y=None):\n",
    "#         print(\"Inside only Get Kernel Function !!!\")\n",
    "#         if callable(self.kernel):\n",
    "#             print(\"Inside only Get Kernel Function (IF) !!!\")\n",
    "#             params = self.kernel_params or {}\n",
    "#         else:\n",
    "#             print(\"Inside only Get Kernel Function (Else) !!!\")\n",
    "#             params = {\"gamma\": self.gamma,\n",
    "#                       \"degree\": self.degree,\n",
    "#                       \"coef0\": self.coef0}\n",
    "#         return pairwise_kernels(X, Y, metric=self.kernel,\n",
    "#                                 filter_params=True, **params)\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "#         print(\"Inside only Fit Function !!!\")\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "#         K = self._get_kernel(X)\n",
    "        K = X\n",
    "\n",
    "        sw = sample_weight if sample_weight else np.ones(n_samples)\n",
    "        self.sample_weight_ = sw\n",
    "\n",
    "        rs = check_random_state(self.random_state)\n",
    "        self.labels_ = rs.randint(self.n_clusters, size=n_samples)\n",
    "\n",
    "        dist = np.zeros((n_samples, self.n_clusters))\n",
    "        self.within_distances_ = np.zeros(self.n_clusters)\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            dist.fill(0)\n",
    "            self._compute_dist(K, dist, self.within_distances_,\n",
    "                               update_within=True)\n",
    "            labels_old = self.labels_\n",
    "            self.labels_ = dist.argmin(axis=1)\n",
    "\n",
    "            # Compute the number of samples whose cluster did not change \n",
    "            # since last iteration.\n",
    "            n_same = np.sum((self.labels_ - labels_old) == 0)\n",
    "            if 1 - float(n_same) / n_samples < self.tol:\n",
    "                if self.verbose:\n",
    "                    print(\"Converged at iteration\", it + 1)\n",
    "                break\n",
    "\n",
    "        self.X_fit_ = X\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _compute_dist(self, K, dist, within_distances, update_within):\n",
    "#         print(\"Inside only Compute Dist Function !!!\")\n",
    "        \"\"\"Compute a n_samples x n_clusters distance matrix using the \n",
    "        kernel trick.\"\"\"\n",
    "        sw = self.sample_weight_\n",
    "\n",
    "        for j in range(self.n_clusters):\n",
    "            mask = self.labels_ == j\n",
    "\n",
    "#             if np.sum(mask) == 0:\n",
    "#                 raise ValueError(\"Empty cluster found, try smaller n_cluster.\")\n",
    "\n",
    "            denom = sw[mask].sum()\n",
    "            denomsq = denom * denom\n",
    "\n",
    "            if update_within:\n",
    "                KK = K[mask][:, mask]  # K[mask, mask] does not work.\n",
    "                dist_j = np.sum(np.outer(sw[mask], sw[mask]) * KK / denomsq)\n",
    "                within_distances[j] = dist_j\n",
    "                dist[:, j] += dist_j\n",
    "            else:\n",
    "                dist[:, j] += within_distances[j]\n",
    "\n",
    "            dist[:, j] -= 2 * np.sum(sw[mask] * K[:, mask], axis=1) / denom\n",
    "\n",
    "    def predict(self, X,kk):\n",
    "#         print(\"Inside only Predict Function !!!\")\n",
    "#         K = self._get_kernel(X, self.X_fit_)\n",
    "        K = kk\n",
    "        n_samples = X.shape[0]\n",
    "        dist = np.zeros((n_samples, self.n_clusters))\n",
    "        self._compute_dist(K, dist, self.within_distances_,\n",
    "                           update_within=False)\n",
    "        return dist.argmin(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =>4, m =>0, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 11\n",
      "k =>4, m =>0, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 21\n",
      "k =>4, m =>0, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 18\n",
      "k =>4, m =>0, Trail =>2, # of Clusters =>6\n",
      "Converged at iteration 22\n",
      "k =>4, m =>1, Trail =>2, # of Clusters =>3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-172-9ca0818ea794>:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dist[:, j] -= 2 * np.sum(sw[mask] * K[:, mask], axis=1) / denom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =>4, m =>1, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 21\n",
      "k =>4, m =>1, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 18\n",
      "k =>4, m =>1, Trail =>2, # of Clusters =>6\n",
      "k =>4, m =>2, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 11\n",
      "k =>4, m =>2, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 21\n",
      "k =>4, m =>2, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 18\n",
      "k =>4, m =>2, Trail =>2, # of Clusters =>6\n",
      "Converged at iteration 32\n",
      "k =>4, m =>3, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 12\n",
      "k =>4, m =>3, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 22\n",
      "k =>4, m =>3, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 22\n",
      "k =>4, m =>3, Trail =>2, # of Clusters =>6\n",
      "Converged at iteration 26\n",
      "k =>6, m =>0, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 9\n",
      "k =>6, m =>0, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 20\n",
      "k =>6, m =>0, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 21\n",
      "k =>6, m =>0, Trail =>2, # of Clusters =>6\n",
      "Converged at iteration 22\n",
      "k =>6, m =>1, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 9\n",
      "k =>6, m =>1, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 20\n",
      "k =>6, m =>1, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 22\n",
      "k =>6, m =>1, Trail =>2, # of Clusters =>6\n",
      "Converged at iteration 23\n",
      "k =>6, m =>2, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 10\n",
      "k =>6, m =>2, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 20\n",
      "k =>6, m =>2, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 24\n",
      "k =>6, m =>2, Trail =>2, # of Clusters =>6\n",
      "k =>6, m =>3, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 11\n",
      "k =>6, m =>3, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 21\n",
      "k =>6, m =>3, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 18\n",
      "k =>6, m =>3, Trail =>2, # of Clusters =>6\n",
      "k =>8, m =>0, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 11\n",
      "k =>8, m =>0, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 19\n",
      "k =>8, m =>0, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 20\n",
      "k =>8, m =>0, Trail =>2, # of Clusters =>6\n",
      "Converged at iteration 23\n",
      "k =>8, m =>1, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 10\n",
      "k =>8, m =>1, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 19\n",
      "k =>8, m =>1, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 23\n",
      "k =>8, m =>1, Trail =>2, # of Clusters =>6\n",
      "Converged at iteration 23\n",
      "k =>8, m =>2, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 10\n",
      "k =>8, m =>2, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 19\n",
      "k =>8, m =>2, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 23\n",
      "k =>8, m =>2, Trail =>2, # of Clusters =>6\n",
      "Converged at iteration 22\n",
      "k =>8, m =>3, Trail =>2, # of Clusters =>3\n",
      "Converged at iteration 11\n",
      "k =>8, m =>3, Trail =>2, # of Clusters =>4\n",
      "Converged at iteration 19\n",
      "k =>8, m =>3, Trail =>2, # of Clusters =>5\n",
      "Converged at iteration 20\n",
      "k =>8, m =>3, Trail =>2, # of Clusters =>6\n",
      "Converged at iteration 24\n"
     ]
    }
   ],
   "source": [
    "Final_agglomorative_confusion_matrix = []\n",
    "Number_of_clusters_arr = [3, 4, 5, 6]\n",
    "trial_arr = [1,2]\n",
    "k_value_arr = [4,6,8]\n",
    "m_value_arr = [0,1,2,3]\n",
    "\n",
    "for data_trials_ind in range(1,len(trial_arr)):\n",
    "    for k_val_ind in range(len(k_value_arr)):\n",
    "        for m_val_ind in range(len(m_value_arr)):\n",
    "            for number_clust_ind in range(len(Number_of_clusters_arr)):\n",
    "\n",
    "                Number_of_clusters = Number_of_clusters_arr[number_clust_ind]\n",
    "                trial = trial_arr[data_trials_ind]\n",
    "                k_value = k_value_arr[k_val_ind]\n",
    "                m_value = m_value_arr[m_val_ind]\n",
    "\n",
    "                holdup_string = \"k =>\" + str(k_value) + \", m =>\" +  str(m_value) + \", Trail =>\" + str(trial) + \", # of Clusters =>\" + str(Number_of_clusters)\n",
    "                print(holdup_string)\n",
    "\n",
    "                #############  Read Kernel Matrix ################\n",
    "                read_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/Kernel_k\" + str(k_value) + \"_m\" + str(m_value) + \"_Alphabet_Size21_trial_\" + str(trial) + \".txt\"\n",
    "\n",
    "\n",
    "                seq_file_small = []\n",
    "                with open(read_path) as csv_file:\n",
    "                    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                    for row in csv_reader:\n",
    "                        seq_file_small.append(', '.join(row))\n",
    "                #############  Read Kernel Matrix ################\n",
    "\n",
    "                #############  Small Kernel Preprocessing ################\n",
    "                final_jernel_mat = []\n",
    "                mat_2 = []\n",
    "                for y in range(0,len(seq_file_small)):\n",
    "                    aa = seq_file_small[y].split(\" \")\n",
    "                    tmp_list = []\n",
    "                    for yy in range(0,len(aa)-1):\n",
    "                        tmp_list.append(int(aa[yy]))\n",
    "\n",
    "                    final_jernel_mat.append(tmp_list)\n",
    "                #############  Small Kernel Preprocessing ################\n",
    "\n",
    "\n",
    "                #############  Kernel k-means ################\n",
    "                X = np.array(final_jernel_mat) #np.array(data_reduced)\n",
    "                km = KernelKMeans(n_clusters=Number_of_clusters, max_iter=50, random_state=0, kernel = \"precomputed\", verbose=1)\n",
    "                fit_var = km.fit_predict(X)\n",
    "                pred_var = km.predict(np.array(data_reduced),np.array(final_jernel_mat))\n",
    "                clusters_ids = pred_var\n",
    "                #############  Kernel k-means ################\n",
    "\n",
    "\n",
    "                #############  Confusion Matrix ################\n",
    "                from sklearn.metrics import confusion_matrix\n",
    "\n",
    "                unique_labels = np.unique(data_labels_reduced)\n",
    "                labels_integers = []\n",
    "                for i in range(len(data_labels_reduced)):\n",
    "                    aa = (np.where(unique_labels==data_labels_reduced[i]))\n",
    "                    labels_integers.append(aa[0][0])\n",
    "\n",
    "                confusion_matrix(clusters_ids,labels_integers)\n",
    "\n",
    "                confuse = confusion_matrix(clusters_ids,labels_integers)\n",
    "\n",
    "                confuse_matrix = []\n",
    "                for i in range(0,len(confuse)):\n",
    "                    confuse_matrix.append(list(confuse[i]))\n",
    "                #############  Confusion Matrix ################\n",
    "\n",
    "                Final_agglomorative_confusion_matrix.append(confuse_matrix)\n",
    "                Final_agglomorative_confusion_matrix.append(holdup_string)\n",
    "#                 #############  Save Confusion Matrix ################\n",
    "#                 unique_labels = np.unique(data_labels_reduced)\n",
    "#                 dataset_save_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/results/Kernel KMeans/trial\" + str(trial) + \"/\"\n",
    "#                 with open(dataset_save_path + \"Kernel_kmeans_confusion_matrix\" + \"_numClust_\" + str(Number_of_clusters) + \"_dataTrial_\" + str(trial) + \"_k_value_\" + str(k_value) + \"_m_value_\" + str(m_value) +  \".csv\", 'w', newline='') as file:\n",
    "#                     writer = csv.writer(file)\n",
    "#                     for i in range(0,Number_of_clusters):#len(confuse)\n",
    "#                         if(i==0):\n",
    "#                             writer.writerow(np.array(unique_labels))\n",
    "#                         writer.writerow(np.array(confuse_matrix[i]))\n",
    "#                 #############  Save Confusion Matrix ################\n",
    "\n",
    "    #############  Save Confusion Matrix ################\n",
    "    unique_labels = np.unique(data_labels_reduced)\n",
    "    dataset_save_path = \"D:/University/RA/covid_origin/Code/Kernel_approximation/results/Kernel KMeans/trial\" + str(trial) + \"/\"\n",
    "    with open(dataset_save_path + \"Final_Kerrnel_PCA_confusion_matrix.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(0,len(Final_agglomorative_confusion_matrix)):#len(confuse)\n",
    "            if(i==0):\n",
    "                writer.writerow(np.array(unique_labels))\n",
    "            if (i % 2) == 0:\n",
    "    #             print(\"{0} is Even number\". format(i))\n",
    "                tmp = Final_agglomorative_confusion_matrix[i]\n",
    "                for u in range(len(tmp)):\n",
    "                    writer.writerow(np.array(tmp[u]))\n",
    "            else:\n",
    "                writer.writerow((Final_agglomorative_confusion_matrix[i]))\n",
    "\n",
    "    #############  Save Confusion Matrix ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clust = pd.DataFrame(clusters_ids),pd.DataFrame(data_labels_reduced))\n",
    "\n",
    "# initialise data of lists.\n",
    "data = {'Cluster IDs':clusters_ids,\n",
    "        'True Labels':data_labels_reduced}\n",
    " \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster IDs</th>\n",
       "      <th>True Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Cluster IDs True Labels\n",
       "0             1       India\n",
       "1             1       India\n",
       "2             3       India\n",
       "3             1       India\n",
       "4             3       India\n",
       "5             1       India\n",
       "6             1       India\n",
       "7             1       India\n",
       "8             1       India\n",
       "9             1       India\n",
       "10            1       India\n",
       "11            0       India\n",
       "12            1       India\n",
       "13            1       India\n",
       "14            1       India\n",
       "15            1       India\n",
       "16            3       India\n",
       "17            5       India\n",
       "18            1       India\n",
       "19            1       India\n",
       "20            1       India\n",
       "21            1       India\n",
       "22            1       India\n",
       "23            3       India\n",
       "24            0       India\n",
       "25            1       India\n",
       "26            1       India\n",
       "27            1       India\n",
       "28            1       India\n",
       "29            1       India\n",
       "30            4       India\n",
       "31            1       India\n",
       "32            1       India\n",
       "33            1       India\n",
       "34            1       India\n",
       "35            1       India\n",
       "36            1       India\n",
       "37            1       India\n",
       "38            1       India\n",
       "39            1       India\n",
       "40            1       India\n",
       "41            1       India\n",
       "42            1       India\n",
       "43            1       India\n",
       "44            1       India\n",
       "45            3       India\n",
       "46            1       India\n",
       "47            1       India\n",
       "48            1       India\n",
       "49            1       India\n",
       "50            1       India\n",
       "51            1       India\n",
       "52            1       India\n",
       "53            1       India\n",
       "54            3       India\n",
       "55            1       India\n",
       "56            1       India\n",
       "57            1       India\n",
       "58            1       India\n",
       "59            1       India"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "read_path = \"D:/University/RA/covid_origin/Dataset/Merged Data/merged_complete_kmers_attributes.csv\"\n",
    "\n",
    "countries_names = []\n",
    "\n",
    "with open(read_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        tmp = row\n",
    "        countries_names.append(tmp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_names_new = []\n",
    "\n",
    "for i in range(len(countries_names)):\n",
    "    tmp = countries_names[i]\n",
    "    countries_names_new.append(tmp.split(\"^\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path_11 = \"D:/University/RA/Visualisation/Clustering_Results/country_names.csv\"\n",
    "\n",
    "with open(write_path_11, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for i in range(0,len(countries_names_new)):\n",
    "        ccv = str(countries_names_new[i])\n",
    "        writer.writerow([ccv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "read_path = \"D:/University/RA/covid_origin/Dataset/Merged Data/merged_complete_kmers_attributes.csv\"\n",
    "\n",
    "countries_names2 = []\n",
    "\n",
    "with open(read_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        tmp = row\n",
    "        countries_names2.append(tmp[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_names_new2 = []\n",
    "\n",
    "for i in range(len(countries_names)):\n",
    "    tmp = countries_names2[i]\n",
    "    countries_names_new2.append(tmp.split(\"/\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path_11 = \"D:/University/RA/Visualisation/Clustering_Results/country_names2.csv\"\n",
    "\n",
    "with open(write_path_11, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for i in range(0,len(countries_names_new2)):\n",
    "        ccv = str(countries_names_new2[i])\n",
    "        writer.writerow([ccv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = \"D:/University/RA/covid_origin/Dataset/Merged Data/merged_complete_kmers_frequency_vectors.csv\"\n",
    "\n",
    "frequency_vector_read = []\n",
    "# host_names_ne = []\n",
    "\n",
    "with open(read_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        tmp = row\n",
    "#         host_names_new.append(', '.join(row))\n",
    "        frequency_vector_read.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frequency_vector_read_int = []\n",
    "\n",
    "for u in range(len(frequency_vector_read)):\n",
    "    aa = frequency_vector_read[u]\n",
    "    test_list = []\n",
    "    for i in range(0, len(aa)):\n",
    "        test_list.append(int(aa[i]))\n",
    "        \n",
    "    frequency_vector_read_int.append(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B.1.1.7',\n",
       " 'B.1.1.7',\n",
       " 'B.1.427',\n",
       " 'B.1.427',\n",
       " 'B.1.427',\n",
       " 'B.1.1.7',\n",
       " 'B.1.1.7',\n",
       " 'B.1.427',\n",
       " 'B.1.427',\n",
       " 'B.1.427']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(host_names_new[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = \"D:/University/RA/covid_origin/Dataset/Merged Data/merged_complete_kmers_attributes.csv\"\n",
    "\n",
    "host_names_new = []\n",
    "host_names_ne = []\n",
    "\n",
    "with open(read_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        tmp = row\n",
    "#         host_names_new.append(', '.join(row))\n",
    "        host_names_new.append(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters =  5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "number_of_clusters = [5]\n",
    "\n",
    "for clust_ind in range(len(number_of_clusters)):\n",
    "    print(\"Number of Clusters = \",number_of_clusters[clust_ind])\n",
    "    clust_num = number_of_clusters[clust_ind]\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=clust_num, random_state=0).fit(frequency_vector_read_int)\n",
    "    kmean_clust_labels = kmeans.labels_\n",
    "\n",
    "\n",
    "    write_path_11 = \"D:/University/RA/Visualisation/Clustering_Results/kmeans_clustering_k_\" + str(clust_num) + \".csv\"\n",
    "\n",
    "    with open(write_path_11, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(0,len(kmean_clust_labels)):\n",
    "            ccv = str(kmean_clust_labels[i])\n",
    "            writer.writerow(ccv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Number:0 \n",
      " 39653 entries in total\n",
      "P.1          21549\n",
      "B.1.1.7       8160\n",
      "B.1.427       6979\n",
      "B.1.617.2     2416\n",
      "B.1.351        549\n",
      "dtype: int64\n",
      "\n",
      " \n",
      "\n",
      "Cluster Number:1 \n",
      " 1904 entries in total\n",
      "B.1.617.2    734\n",
      "B.1.1.7      465\n",
      "B.1.427      374\n",
      "P.1          228\n",
      "B.1.351      103\n",
      "dtype: int64\n",
      "\n",
      " \n",
      "\n",
      "Cluster Number:2 \n",
      " 7635 entries in total\n",
      "B.1.427      2431\n",
      "B.1.1.7      2093\n",
      "P.1          2043\n",
      "B.1.617.2     849\n",
      "B.1.351       219\n",
      "dtype: int64\n",
      "\n",
      " \n",
      "\n",
      "Cluster Number:3 \n",
      " 341 entries in total\n",
      "B.1.427      184\n",
      "B.1.1.7       63\n",
      "P.1           45\n",
      "B.1.351       25\n",
      "B.1.617.2     24\n",
      "dtype: int64\n",
      "\n",
      " \n",
      "\n",
      "Cluster Number:4 \n",
      " 3254 entries in total\n",
      "B.1.617.2    914\n",
      "B.1.1.7      860\n",
      "B.1.427      720\n",
      "P.1          470\n",
      "B.1.351      290\n",
      "dtype: int64\n",
      "\n",
      " \n",
      "\n",
      "Cluster Number:5 \n",
      " 1598 entries in total\n",
      "P.1          561\n",
      "B.1.1.7      345\n",
      "B.1.427      312\n",
      "B.1.617.2    276\n",
      "B.1.351      104\n",
      "dtype: int64\n",
      "\n",
      " \n",
      "\n",
      "Cluster Number:6 \n",
      " 3562 entries in total\n",
      "B.1.617.2    1904\n",
      "B.1.1.7       774\n",
      "B.1.427       390\n",
      "P.1           330\n",
      "B.1.351       164\n",
      "dtype: int64\n",
      "\n",
      " \n",
      "\n",
      "Cluster Number:7 \n",
      " 4710 entries in total\n",
      "P.1          1403\n",
      "B.1.427      1394\n",
      "B.1.1.7      1206\n",
      "B.1.617.2     434\n",
      "B.1.351       273\n",
      "dtype: int64\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_clust_num = np.unique(kmean_clust_labels)\n",
    "\n",
    "for t in range(len(unique_clust_num)):\n",
    "    variants_in_clusters = []\n",
    "    clust_val = unique_clust_num[t]\n",
    "    for y in range(len(kmean_clust_labels)):\n",
    "        if kmean_clust_labels[y]==clust_val:\n",
    "            variants_in_clusters.append(host_names_new[y])       \n",
    "    idx = pd.Index(variants_in_clusters) # creates an index which allows counting the entries easily\n",
    "    print('Cluster Number:' + str(clust_val) + ' \\n', len(idx),\"entries in total\")\n",
    "    aq = (idx.value_counts())\n",
    "    print(aq)\n",
    "    \n",
    "    print('\\n \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45511092, -0.45965061, -0.48299786, ..., -0.64613436,\n",
       "        0.11875082,  0.574086  ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.silhouette_samples(frequency_vector_read_int, host_names_new, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-573e77105945>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgoodness_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequency_vector_read_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhost_names_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    232\u001b[0m     reduce_func = functools.partial(_silhouette_reduce,\n\u001b[0;32m    233\u001b[0m                                     labels=labels, label_freqs=label_freqs)\n\u001b[1;32m--> 234\u001b[1;33m     results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func,\n\u001b[0m\u001b[0;32m    235\u001b[0m                                               **kwds))\n\u001b[0;32m    236\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1614\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1616\u001b[1;33m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0m\u001b[0;32m   1617\u001b[0m                                      n_jobs=n_jobs, **kwds)\n\u001b[0;32m   1618\u001b[0m         if ((X is Y or Y is None)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   1777\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1779\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1360\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;31m# Ensure that distances between vectors and themselves are set to 0.0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "goodness_values = sklearn.metrics.silhouette_samples(frequency_vector_read_int, host_names_new, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-0e824660fce6>:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cluster = cluster / float(cluster.sum())\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-0e824660fce6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mentropy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "entropy, purity = [], []\n",
    "for cluster in kmean_clust_labels:\n",
    "    cluster = np.array(cluster)\n",
    "    cluster = cluster / float(cluster.sum())\n",
    "\n",
    "    e = (cluster * [log(x, 2) for x in cluster]).sum()\n",
    "    p = cluster.max()\n",
    "    entropy += [e]\n",
    "    purity += [p]\n",
    "\n",
    "counts = np.array([c.sum() for c in clusters])\n",
    "coeffs = counts / float(counts.sum())\n",
    "print('entropy: ', (coeffs * entropy).sum())\n",
    "print('purity: ', (coeffs * purity).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62657"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(host_names_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = \"D:/University/RA/Visualisation/Clustering_Results/kmeans_only.csv\"\n",
    "\n",
    "clusters_5 = []\n",
    "\n",
    "with open(read_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        tmp = row\n",
    "#         host_names_new.append(', '.join(row))\n",
    "        clusters_5.append(int(tmp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 0, 4, 0, 0]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(clusters_5[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_names_integers = []\n",
    "for t in range(len(host_names_new)):\n",
    "    if(host_names_new[t]==\"P.1\"):\n",
    "        host_names_integers.append(1)\n",
    "    elif(host_names_new[t]==\"B.1.1.7\"):\n",
    "        host_names_integers.append(2)\n",
    "    elif(host_names_new[t]==\"B.1.427\"):\n",
    "        host_names_integers.append(3)\n",
    "    elif(host_names_new[t]==\"B.1.617.2\"):\n",
    "        host_names_integers.append(4)\n",
    "    elif(host_names_new[t]==\"B.1.351\"):\n",
    "        host_names_integers.append(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_0 = []\n",
    "clust_1 = []\n",
    "clust_2 = []\n",
    "clust_3 = []\n",
    "clust_4 = []\n",
    "\n",
    "for t in range(len(clusters_5)):\n",
    "    if(clusters_5[t]==0):\n",
    "        clust_0.append(host_names_integers[t])\n",
    "    elif(clusters_5[t]==1):\n",
    "        clust_1.append(host_names_integers[t])\n",
    "    elif(clusters_5[t]==2):\n",
    "        clust_2.append(host_names_integers[t])\n",
    "    elif(clusters_5[t]==3):\n",
    "        clust_3.append(host_names_integers[t])\n",
    "    elif(clusters_5[t]==4):\n",
    "        clust_4.append(host_names_integers[t])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6151"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clust_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are all of the viral species in the dataset: \n",
      " 62657 entries in total\n",
      "1    26629\n",
      "2    13966\n",
      "3    12784\n",
      "4     7551\n",
      "5     1727\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "idx = pd.Index(host_names_integers) # creates an index which allows counting the entries easily\n",
    "print('Here are all of the viral species in the dataset: \\n', len(idx),\"entries in total\")\n",
    "aq = (idx.value_counts())\n",
    "print(aq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are all of the viral species in the dataset: \n",
      " 62657 entries in total\n",
      "P.1          26629\n",
      "B.1.1.7      13966\n",
      "B.1.427      12784\n",
      "B.1.617.2     7551\n",
      "B.1.351       1727\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "idx = pd.Index(host_names_new) # creates an index which allows counting the entries easily\n",
    "print('Here are all of the viral species in the dataset: \\n', len(idx),\"entries in total\")\n",
    "aq = (idx.value_counts())\n",
    "print(aq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 2706), (2, 1512), (3, 956), (1, 682), (5, 295)]\n",
      "[(1, 22140), (2, 8762), (3, 7848), (4, 2605), (5, 601)]\n",
      "[(3, 187), (2, 86), (1, 50), (5, 33), (4, 30)]\n",
      "[(4, 868), (1, 741), (2, 680), (3, 638), (5, 172)]\n",
      "[(3, 3155), (1, 3016), (2, 2926), (4, 1342), (5, 626)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Elements_with_frequency = Counter(clust_0)\n",
    "print(Elements_with_frequency.most_common())\n",
    "\n",
    "Elements_with_frequency = Counter(clust_1)\n",
    "print(Elements_with_frequency.most_common())\n",
    "\n",
    "Elements_with_frequency = Counter(clust_2)\n",
    "print(Elements_with_frequency.most_common())\n",
    "\n",
    "Elements_with_frequency = Counter(clust_3)\n",
    "print(Elements_with_frequency.most_common())\n",
    "\n",
    "Elements_with_frequency = Counter(clust_4)\n",
    "print(Elements_with_frequency.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[   0    0    0    0    0]\n",
      " [   0    0    0    0    0]\n",
      " [   0    0    0    0    0]\n",
      " [ 682 1512  956 2706  295]\n",
      " [   0    0    0    0    0]]\n",
      "Accuracy :  0.4399284669159486\n",
      "Sensitivity :  0.0\n",
      "Specificity :  0.0\n",
      "[0.6110421135824771, 0.6908387418871692, 0.6527050610820244, 0.3598835670812385, 0.10709092464288768]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clust_0_true =  [4] * len(clust_0)\n",
    "f1_0 = metrics.f1_score(clust_0_true, clust_0,average='weighted')\n",
    "cm1 = confusion_matrix(clust_0_true,clust_0)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "\n",
    "total1=sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm1[0,0]+cm1[1,1]+cm1[2,2]+cm1[3,3]+cm1[4,4])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1] + 0.0001)\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1] + 0.0001)\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "\n",
    "clust_1_true =  [1] * len(clust_1)\n",
    "f1_1 = metrics.f1_score(clust_1_true, clust_1,average='weighted')\n",
    "\n",
    "clust_2_true =  [3] * len(clust_2)\n",
    "f1_2 = metrics.f1_score(clust_2_true, clust_2,average='weighted')\n",
    "\n",
    "clust_3_true =  [2] * len(clust_3)\n",
    "f1_3 = metrics.f1_score(clust_3_true, clust_3,average='weighted')\n",
    "\n",
    "clust_4_true =  [5] * len(clust_4)\n",
    "f1_4 = metrics.f1_score(clust_4_true, clust_4,average='weighted')\n",
    "\n",
    "print([f1_0,f1_1,f1_2,f1_3,f1_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are all of the viral species in the dataset: \n",
      " 26433 entries in total\n",
      "1    26433\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "idx = pd.Index(clust_0_true) # creates an index which allows counting the entries easily\n",
    "print('Here are all of the viral species in the dataset: \\n', len(idx),\"entries in total\")\n",
    "aq = (idx.value_counts())\n",
    "print(aq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are all of the viral species in the dataset: \n",
      " 26433 entries in total\n",
      "1    26426\n",
      "2        6\n",
      "5        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "idx = pd.Index(clust_0) # creates an index which allows counting the entries easily\n",
    "print('Here are all of the viral species in the dataset: \\n', len(idx),\"entries in total\")\n",
    "aq = (idx.value_counts())\n",
    "print(aq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
